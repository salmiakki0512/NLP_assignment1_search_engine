{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment1 Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary library\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import time\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use brown corpus dataset (author: W. N. Francis and H. Kucera)from https://www.nltk.org/nltk_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. tokenization\n",
    "corpus = brown.sents(categories=['news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. numeralization\n",
    "#find unique words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "#assign unique integer\n",
    "vocabs = list(set(flatten(corpus))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14394"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there are 14394 words in the vocabs\n",
    "len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping between integer and word\n",
    "word2index = {v:idx for idx, v in enumerate(vocabs)}\n",
    "index2word = {v:k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add <UNK> as last index in the vocabs\n",
    "vocabs.append('<UNK>')\n",
    "word2index['<UNK>'] = 14394"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Co-occurence Matrix X\n",
    "\n",
    "for glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "X_i = Counter(flatten(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_grams = []\n",
    "\n",
    "for doc in corpus:\n",
    "    for i in range(2, len(doc)-2):\n",
    "        center = doc[i]\n",
    "        outside = [doc[i-2],doc[i-1], doc[i+1],doc[i-2]]\n",
    "        for each_out in outside:\n",
    "            skip_grams.append((center, each_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ik_skipgrams = Counter(skip_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting function\n",
    "\n",
    "GloVe includes a weighting function to scale down too frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting(w_i, w_j, X_ik):\n",
    "    \n",
    "    #check whether the co-occurences between w_i and w_j is available\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "    except:\n",
    "        #if not exist, then set to 1 \"laplace smoothing\"\n",
    "        x_ij = 1\n",
    "        \n",
    "    #set xmax\n",
    "    x_max = 100\n",
    "    #set alpha\n",
    "    alpha = 0.75\n",
    "    \n",
    "    #if co-ocurrence does not exceeed xmax, then just multiply with some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij / x_max)**alpha\n",
    "    #otherwise, set to 1\n",
    "    else:\n",
    "        result = 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {} #keeping the co-occurences\n",
    "weighting_dic = {} #scale the co-occurences using the weighting function\n",
    "\n",
    "for bigram in combinations_with_replacement(vocabs, 2):\n",
    "    if X_ik_skipgrams.get(bigram):  #if the pair exists in our corpus\n",
    "        co = X_ik_skipgrams[bigram]\n",
    "        X_ik[bigram] = co + 1 #for stability\n",
    "        X_ik[(bigram[1], bigram[0])] = co + 1 #basically apple, banana = banana, apple\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pairs of center word, and outside word for skipgram model\n",
    "\n",
    "def random_batch(batch_size, corpus):\n",
    "\n",
    "    skipgrams = []\n",
    "\n",
    "    #loop each corpus\n",
    "    for doc in corpus:\n",
    "        #use window size=2\n",
    "        for i in range(2, len(doc)-2):\n",
    "            #center word\n",
    "            center = word2index[doc[i]]\n",
    "            #outside words = 4 words\n",
    "            outside = (word2index[doc[i-2]],word2index[doc[i-1]], word2index[doc[i+1]],word2index[doc[i+2]],)\n",
    "            #append outside words to a list\n",
    "            for each_out in outside:\n",
    "                skipgrams.append([center, each_out])\n",
    "                \n",
    "    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace=False)\n",
    "    \n",
    "    inputs, labels = [], []\n",
    "    for index in random_index:\n",
    "        inputs.append([skipgrams[index][0]])\n",
    "        labels.append([skipgrams[index][1]])\n",
    "        \n",
    "    return np.array(inputs), np.array(labels)\n",
    "            \n",
    "# x, y = random_batch(2, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random batch for glove model\n",
    "def random_batch_glove(batch_size, word_sequence, skip_grams, X_ik, weighting_dic):\n",
    "    \n",
    "    random_inputs, random_labels, random_coocs, random_weightings = [], [], [], []\n",
    "    \n",
    "    #convert our skipgrams to id\n",
    "    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    \n",
    "    #randomly choose indexes based on batch size\n",
    "    random_index = np.random.choice(range(len(skip_grams_id)), batch_size, replace=False)\n",
    "    \n",
    "    #get the random input and labels\n",
    "    for index in random_index:\n",
    "        random_inputs.append([skip_grams_id[index][0]])\n",
    "        random_labels.append([skip_grams_id[index][1]])\n",
    "        #coocs\n",
    "        pair = skip_grams[index] \n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1\n",
    "        random_coocs.append([math.log(cooc)])\n",
    "    \n",
    "        #weightings\n",
    "        weighting = weighting_dic[pair]\n",
    "        random_weightings.append([weighting])\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count\n",
    "from collections import Counter\n",
    "\n",
    "word_count = Counter(flatten(corpus))\n",
    "word_count\n",
    "\n",
    "#get the total number of words\n",
    "num_total_words = sum([c for w, c in word_count.items()])\n",
    "# num_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_table = []\n",
    "z=0.01\n",
    "for v in vocabs:\n",
    "    uw = word_count[v] / num_total_words\n",
    "    uw_alpha = int((uw ** 0.75) / z)\n",
    "    unigram_table.extend([v] * uw_alpha)\n",
    "    \n",
    "# Counter(unigram_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are 14395 words in vocabs\n",
    "len(vocabs)\n",
    "embedding = nn.Embedding(14395, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_tensor = torch.LongTensor(x)\n",
    "# embedding(x_tensor).shape  #(batch_size, 1, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        center_embedding     = self.embedding_center(center)  #(batch_size, 1, emb_size)\n",
    "        outside_embedding    = self.embedding_center(outside) #(batch_size, 1, emb_size)\n",
    "        all_vocabs_embedding = self.embedding_center(all_vocabs) #(batch_size, voc_size, emb_size)\n",
    "        \n",
    "        top_term = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        #batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1) \n",
    "\n",
    "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        #batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size) \n",
    "        \n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1)  #(batch_size, 1)\n",
    "        \n",
    "        loss = -torch.mean(torch.log(top_term / lower_term_sum))  #scalar\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    #add predict function to predict the model\n",
    "    def predict(self, A, B, C):\n",
    "        # Given three words (A, B, C), return their embeddings\n",
    "        A_idx = torch.tensor([word2index[A]])  # Convert words to indices\n",
    "        B_idx = torch.tensor([word2index[B]])\n",
    "        C_idx = torch.tensor([word2index[C]])\n",
    "\n",
    "        A_embedding = self.embedding_center(A_idx)\n",
    "        B_embedding = self.embedding_center(B_idx)\n",
    "        C_embedding = self.embedding_center(C_idx)\n",
    "        \n",
    "        return A_embedding, B_embedding, C_embedding\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare all vocabs\n",
    "\n",
    "batch_size = 2\n",
    "voc_size   = len(vocabs)\n",
    "\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.shape[0]\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):  #(1, k)\n",
    "        target_index = targets[i].item()\n",
    "        nsample      = []\n",
    "        while (len(nsample) < k):\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).reshape(1, -1))\n",
    "        \n",
    "    return torch.cat(neg_samples) #batch_size, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramNeg(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid        = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, center, outside, negative):\n",
    "        #center, outside:  (bs, 1)\n",
    "        #negative       :  (bs, k)\n",
    "        \n",
    "        center_embed   = self.embedding_center(center) #(bs, 1, emb_size)\n",
    "        outside_embed  = self.embedding_outside(outside) #(bs, 1, emb_size)\n",
    "        negative_embed = self.embedding_outside(negative) #(bs, k, emb_size)\n",
    "        \n",
    "        uovc           = outside_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, 1)\n",
    "        ukvc           = -negative_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, k)\n",
    "        ukvc_sum       = torch.sum(ukvc, 1).reshape(-1, 1) #(bs, 1)\n",
    "        \n",
    "        loss           = self.logsigmoid(uovc) + self.logsigmoid(ukvc_sum)\n",
    "        \n",
    "        return -torch.mean(loss)\n",
    "\n",
    "    #predict function\n",
    "    def predict(self, A, B, C):\n",
    "        # Given three words (A, B, C), return their embeddings\n",
    "\n",
    "        A_idx = torch.tensor([word2index[A]])  # Convert words to indices\n",
    "        B_idx = torch.tensor([word2index[B]])\n",
    "        C_idx = torch.tensor([word2index[C]])\n",
    "\n",
    "        A_embedding = self.embedding_center(A_idx)\n",
    "        B_embedding = self.embedding_center(B_idx)\n",
    "        C_embedding = self.embedding_center(C_idx)\n",
    "        \n",
    "        return A_embedding, B_embedding, C_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glove(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Glove, self).__init__()\n",
    "        self.center_embedding  = nn.Embedding(voc_size, emb_size)\n",
    "        self.outside_embedding = nn.Embedding(voc_size, emb_size)\n",
    "        \n",
    "        self.center_bias       = nn.Embedding(voc_size, 1) \n",
    "        self.outside_bias      = nn.Embedding(voc_size, 1)\n",
    "    \n",
    "    def forward(self, center, outside, coocs, weighting):\n",
    "        center_embeds  = self.center_embedding(center) #(batch_size, 1, emb_size)\n",
    "        outside_embeds = self.outside_embedding(outside) #(batch_size, 1, emb_size)\n",
    "        \n",
    "        center_bias    = self.center_bias(center).squeeze(1)\n",
    "        target_bias    = self.outside_bias(outside).squeeze(1)\n",
    "        \n",
    "        inner_product  = outside_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #(batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1)\n",
    "        \n",
    "        loss = weighting * torch.pow(inner_product + center_bias + target_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)\n",
    "\n",
    "    def predict(self, A, B, C):\n",
    "        # Given three words (A, B, C), return their embeddings\n",
    "\n",
    "        A_idx = torch.tensor([word2index[A]])  # Convert words to indices\n",
    "        B_idx = torch.tensor([word2index[B]])\n",
    "        C_idx = torch.tensor([word2index[C]])\n",
    "\n",
    "        A_embedding = self.center_embedding(A_idx)\n",
    "        B_embedding = self.center_embedding(B_idx)\n",
    "        C_embedding = self.center_embedding(C_idx)\n",
    "        \n",
    "        return A_embedding, B_embedding, C_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "emb_size   = 2\n",
    "#skipgram\n",
    "model1      = Skipgram(voc_size, emb_size)\n",
    "#skipgram negative\n",
    "model2      = SkipgramNeg(voc_size, emb_size)\n",
    "#glove\n",
    "model3      = Glove(voc_size, emb_size)\n",
    "#use Adam as optimizer and use crossentropyloss\n",
    "optimizer   = optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer   = optim.Adam(model2.parameters(), lr=0.001)\n",
    "optimizer   = optim.Adam(model3.parameters(), lr=0.001)\n",
    "criterion   = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Skipgram\n",
      "Epoch    100 | Loss: 11.108609 \n",
      "Epoch    200 | Loss: 9.449618 \n",
      "Epoch    300 | Loss: 9.931557 \n",
      "Epoch    400 | Loss: 9.949294 \n",
      "Epoch    500 | Loss: 13.314457 \n",
      "Epoch    600 | Loss: 10.792645 \n",
      "Epoch    700 | Loss: 9.351409 \n",
      "Epoch    800 | Loss: 9.354146 \n",
      "Epoch    900 | Loss: 13.083500 \n",
      "Epoch   1000 | Loss: 9.689467 \n",
      "time: 10m 33s\n",
      "--------------------\n",
      "Model: Skipgram Negative Sampling\n",
      "Epoch    100 | Loss: 2.268116 \n",
      "Epoch    200 | Loss: 1.914912 \n",
      "Epoch    300 | Loss: 3.088072 \n",
      "Epoch    400 | Loss: 1.093648 \n",
      "Epoch    500 | Loss: 1.268568 \n",
      "Epoch    600 | Loss: 0.778361 \n",
      "Epoch    700 | Loss: 3.887585 \n",
      "Epoch    800 | Loss: 3.546030 \n",
      "Epoch    900 | Loss: 1.530818 \n",
      "Epoch   1000 | Loss: 1.979582 \n",
      "time: 10m 28s\n",
      "--------------------\n",
      "Model: Glove\n",
      "Epoch    100 | Loss: 1.102688 \n",
      "Epoch    200 | Loss: 0.432289 \n",
      "Epoch    300 | Loss: 0.366801 \n",
      "Epoch    400 | Loss: 10.537242 \n",
      "Epoch    500 | Loss: 0.557471 \n",
      "Epoch    600 | Loss: 0.035286 \n",
      "Epoch    700 | Loss: 1.085876 \n",
      "Epoch    800 | Loss: 0.039938 \n",
      "Epoch    900 | Loss: 25.407887 \n",
      "Epoch   1000 | Loss: 0.303225 \n",
      "time: 1m 49s\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "\n",
    "start1 = time.time()\n",
    "print(\"Model: Skipgram\")\n",
    "for epoch in range(num_epochs):\n",
    "    #get batch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus)\n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "    #predict\n",
    "    loss1 = model1(input_tensor, label_tensor, all_vocabs)\n",
    "    #backprogate\n",
    "    optimizer.zero_grad()\n",
    "    loss1.backward()\n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    #print the loss\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1:6.0f} | Loss: {loss1:2.6f} \")\n",
    "end1 = time.time()\n",
    "epoch_mins1, epoch_secs1 = epoch_time(start1, end1)\n",
    "print(f\"time: {epoch_mins1}m {epoch_secs1}s\")\n",
    "print(\"-\"*20)\n",
    "\n",
    "start2 = time.time()\n",
    "print(\"Model: Skipgram Negative Sampling\")\n",
    "for epoch in range(num_epochs):\n",
    "    #get batch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus)\n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "    #predict\n",
    "    k=5\n",
    "    neg_samples = negative_sampling(label_tensor, unigram_table, k)\n",
    "    loss2 = model2(input_tensor, label_tensor, neg_samples)\n",
    "\n",
    "    #backprogate\n",
    "    optimizer.zero_grad()\n",
    "    loss2.backward()\n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1:6.0f} | Loss: {loss2:2.6f} \")\n",
    "end2 = time.time()\n",
    "epoch_mins2, epoch_secs2 = epoch_time(start2, end2)\n",
    "print(f\"time: {epoch_mins2}m {epoch_secs2}s\")\n",
    "print(\"-\"*20)\n",
    "\n",
    "start3 = time.time()\n",
    "print(\"Model: Glove\")\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    input_batch, target_batch, cooc_batch, weighting_batch = random_batch_glove(batch_size, corpus, skip_grams, X_ik, weighting_dic)\n",
    "    input_batch  = torch.LongTensor(input_batch)         #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch)        #[batch_size, 1]\n",
    "    cooc_batch   = torch.FloatTensor(cooc_batch)         #[batch_size, 1]\n",
    "    weighting_batch = torch.FloatTensor(weighting_batch) #[batch_size, 1]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss3 = model3(input_batch, target_batch, cooc_batch, weighting_batch)\n",
    "    \n",
    "    loss3.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1:6.0f} | Loss: {loss3:2.6f} \")\n",
    "end3 = time.time()\n",
    "epoch_mins3, epoch_secs3 = epoch_time(start3, end3)\n",
    "print(f\"time: {epoch_mins3}m {epoch_secs3}s\")\n",
    "print(\"-\"*20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data source: https://www.fit.vutbr.cz/~imikolov/rnnlm/word-test.v1.txt\n",
    "for semantic.txt: capital-common-countries\n",
    "for syntacticdataset.txt: past-tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import analogies dataset in order to calculate syntactic and semantic accuracy\n",
    "analogies_df = pd.read_csv('syntacticdataset.txt', header=None)\n",
    "analogies_df[['1','2','3','4']] = analogies_df[0].str.split(' ', expand=True)\n",
    "analogies_df.drop([0], axis='columns', inplace=True)\n",
    "analogies_df=analogies_df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogies_df2= pd.read_csv('semantic.txt', header=None)\n",
    "analogies_df2[['1','2','3','4']] = analogies_df2[0].str.split(' ', expand=True)\n",
    "analogies_df2.drop([0], axis='columns', inplace=True)\n",
    "analogies_df2=analogies_df2.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#syntactic accuracy function\n",
    "def calculate_syntactic_accuracy(model, analogy_dataset):\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for analogy_question in analogy_dataset:\n",
    "        try:\n",
    "\n",
    "            A, B, C, correct_answer = analogy_question  # extract components into question and answer model should predict\n",
    "            if model == 'model_gensim':\n",
    "                predicted_answer = model.most_similar(positive=[A, B, C])\n",
    "            else:\n",
    "                predicted_answer = model.predict(A, B, C)  # predict the answer from previous models\n",
    "\n",
    "                if predicted_answer == correct_answer:\n",
    "                    correct_predictions += 1\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    syntactic_accuracy = (correct_predictions / len(analogy_dataset)) * 100\n",
    "    return syntactic_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#semantic accuracy function\n",
    "def calculate_semantic_accuracy(model, analogy_dataset, vocab):\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for analogy_question in analogy_dataset:\n",
    "        try:\n",
    "            A, B, C, correct_answer = analogy_question \n",
    "\n",
    "            # Check if words are in the vocabulary before using them\n",
    "            if (A in word2index) and (B in word2index) and( C in word2index):\n",
    "                A_idx, B_idx, C_idx = word2index[A], word2index[B], word2index[C]\n",
    "\n",
    "                # Predict the word that completes the analogy\n",
    "                predicted_answer = model.predict(A_idx, B_idx, C_idx)\n",
    "\n",
    "                # Convert the predicted answer index to the corresponding word\n",
    "                predicted_word = next(key for key, value in word2index.items() if value == predicted_answer.item())\n",
    "\n",
    "                if predicted_word == correct_answer:\n",
    "                    correct_predictions += 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    semantic_accuracy = (correct_predictions / len(analogy_dataset)) * 100\n",
    "    return semantic_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntactic Accuracy Skipgram: 0.0%\n",
      "Syntactic Accuracy SkipgramNeg: 0.0%\n",
      "Syntactic Accuracy Glove: 0.0%\n"
     ]
    }
   ],
   "source": [
    "syntactic_accuracy1 =calculate_syntactic_accuracy(model1, analogies_df)\n",
    "syntactic_accuracy2 =calculate_syntactic_accuracy(model2, analogies_df)\n",
    "syntactic_accuracy3 =calculate_syntactic_accuracy(model3, analogies_df)\n",
    "print(f\"Syntactic Accuracy Skipgram: {syntactic_accuracy1}%\")\n",
    "print(f\"Syntactic Accuracy SkipgramNeg: {syntactic_accuracy2}%\")\n",
    "print(f\"Syntactic Accuracy Glove: {syntactic_accuracy3}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Accuracy: 0.0%\n",
      "Semantic Accuracy2: 0.0%\n",
      "Semantic Accuracy3: 0.0%\n"
     ]
    }
   ],
   "source": [
    "semantic_accuracy = calculate_semantic_accuracy(model1, analogies_df2, word2index)\n",
    "semantic_accuracy2 = calculate_semantic_accuracy(model2, analogies_df2, word2index)\n",
    "semantic_accuracy3 = calculate_semantic_accuracy(model3, analogies_df2, word2index)\n",
    "print(f\"Semantic Accuracy: {semantic_accuracy}%\")\n",
    "print(f\"Semantic Accuracy2: {semantic_accuracy2}%\")\n",
    "print(f\"Semantic Accuracy3: {semantic_accuracy3}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "I selected Glove model because the model have the lowest lost and spend the fewest time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(model, word):\n",
    "    id_tensor = torch.LongTensor([word2index[word]])\n",
    "    v_embed = model.center_embedding(id_tensor)\n",
    "    u_embed = model.outside_embedding(id_tensor)\n",
    "    word_embed = (v_embed + u_embed) / 2\n",
    "    x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "def cos_sim(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source of ttb_wiki data: https://en.wikipedia.org/wiki/The_Big_Bang_Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Big Bang Theory is an American television sitcom created by Chuck Lorre and Bill Prady, both of whom served as executive producers and head writers on the series, along with Steven Molaro', ' It aired on CBS from September 24, 2007, to May 16, 2019, running for 12 seasons and 279 episodes', \"[3]  The show originally centered on five characters living in Pasadena, California: Leonard Hofstadter (Johnny Galecki) and Sheldon Cooper (Jim Parsons), both physicists at Caltech, who share an apartment; Penny (Kaley Cuoco), a waitress and aspiring actress who lives across the hall; and Leonard and Sheldon's similarly geeky and socially awkward friends and coworkers, aerospace engineer Howard Wolowitz (Simon Helberg) and astrophysicist Raj Koothrappali (Kunal Nayyar)\", '[4][5] Over time, supporting characters were promoted to starring roles, including neuroscientist Amy Farrah Fowler (Mayim Bialik), microbiologist Bernadette Rostenkowski (Melissa Rauch), and comic book store owner Stuart Bloom (Kevin Sussman)', '  The show was filmed in front of a live audience and produced by Chuck Lorre Productions and Warner Bros', ' Television', ' It received mixed reviews throughout its first season, but reception was more favorable in the second and third seasons', ' Despite early mixed reviews, seven seasons were ranked within the top ten of the final season ratings, and it ultimately reached the no', ' 1 spot in its eleventh season', ' It was nominated for the Emmy Award for Outstanding Comedy Series from 2011 to 2014 and won the Emmy Award for Outstanding Lead Actor in a Comedy Series four times for Parsons, totaling seven Emmy Awards from 46 nominations', ' Parsons also won the Golden Globe for Best Actor in a Television Comedy Series in 2011', \" A prequel series, Young Sheldon, based on Parsons' character Sheldon Cooper, premiered in 2017, with Parsons reprising his role as the narrating adult Sheldon\", ' Another spinoff is in development', ' Cast and characters Main article: List of The Big Bang Theory and Young Sheldon characters  This section describes a work or element of fiction in a primarily in-universe style', ' Please help rewrite it to explain the fiction more clearly and provide non-fictional perspective', ' (February 2017) (Learn how and when to remove this template message)  Main characters in The Big Bang Theory season 6', ' First row from left: Rajesh Koothrappali, Leonard Hofstadter, Penny, Sheldon Cooper, and Howard Wolowitz', ' Second row from left: Bernadette Rostenkowski-Wolowitz and Amy Farrah Fowler', ' Johnny Galecki as Leonard Hofstadter:[6] An experimental physicist with an IQ of 173, who received his Ph', 'D', ' when he was 24 years old', ' Leonard is a nerd who loves video games, comic books, and Dungeons & Dragons', ' Leonard is the straight man of the series, sharing an apartment in Pasadena, CA, with Sheldon Cooper', ' Leonard is smitten with his new neighbor Penny when they first meet, and they eventually marry', ' Jim Parsons as Sheldon Cooper:[7] Originally from Galveston, Texas, Sheldon was a child prodigy with an eidetic memory who began college at the age of eleven and earned a Ph', 'D', ' at age sixteen', ' He is a theoretical physicist researching quantum mechanics and string theory, and, despite his IQ of 187, he finds many routine aspects of social situations difficult to grasp', ' He is determined to have his own way, continually boasts of his intelligence, and has an extremely ritualized way of living', ' Despite these quirks, he begins a relationship with Amy Farrah Fowler, and they eventually marry', ' Kaley Cuoco as Penny:[8] An aspiring actress from Omaha, Nebraska', ' Penny moves in across the hall from Sheldon and Leonard', ' She waits tables and occasionally tends the bar at The Cheesecake Factory', ' After giving up hope of becoming a successful actress, Penny becomes a pharmaceutical sales representative', \" Penny becomes friends with Bernadette and Amy, and they often hang out in each other's apartments\", ' Penny and Leonard form a relationship and eventually marry', \" Simon Helberg as Howard Wolowitz:[9] An aerospace engineer who got his master's degree at the Massachusetts Institute of Technology\", ' Howard is Jewish and lived with his mother, Debbie (Carol Ann Susi)', ' Unlike Sheldon, Leonard, Raj, Bernadette, and Amy, Howard does not hold a doctorate', ' He trains as an astronaut and goes into space as a payload specialist on the International Space Station', ' Howard initially fancies himself as a ladies man, but he later starts dating Bernadette, and they get engaged and married', ' Howard also has a tendency to waste money on toys and argues with Bernadette because of his oddly low income as an engineer and her high income as a pharmaceutical biochemist', ' Kunal Nayyar as Rajesh Koothrappali:[10] A particle astrophysicist originally from New Delhi, India', ' Initially, Raj had selective mutism, rendering him unable to talk to or be around women unless under the influence of alcohol', ' Raj also has very feminine tastes and often takes on a stereotypical female role in his friendship with Howard as well as in the group of four men', ' Raj later dates Lucy (Kate Micucci), who also suffers from social anxiety, but it eventually ends', ' He later speaks to Penny without alcohol, overcoming his selective mutism', ' He begins dating Emily Sweeney, and their relationship later becomes exclusive', \" In the series' final season, Raj has an on-again, off-again engagement with a fellow Indian, a hotel concierge named Anu (Rati Gupta)\", ' He also has a Yorkshire Terrier named Cinnamon', ' Sara Gilbert as Leslie Winkle (recurring season 1, starring season 2, guest seasons 3, 9):[11][12][13] A physicist who works in the same lab as Leonard', \" In appearance, she is essentially Leonard's female counterpart and has conflicting scientific theories with Sheldon\", ' Leslie has casual sex with Leonard and later Howard', ' Gilbert was promoted to a main cast member during the second season but resumed guest star status because producers could not come up with enough material for the character', '[11] Gilbert returned to The Big Bang Theory for its 200th episode', '[14] Melissa Rauch as Bernadette Rostenkowski-Wolowitz (recurring season 3, starring seasons 4â€“12):[15] A young woman who initially is a co-worker at The Cheesecake Factory with Penny to pay her way through graduate school, where she is studying microbiology', ' Bernadette is introduced to Howard by Penny; at first, they do not get along, apparently having nothing in common', ' They date and later get engaged and married', ' Although generally a sweet and good-natured person, Bernadette has a short fuse and can be vindictive and lash out when provoked', \" Mayim Bialik as Amy Farrah Fowler (guest star season 3, starring seasons 4â€“12):[16] A woman selected by an online dating site as Sheldon's perfect mate,[17] Amy is from Glendale, California\", ' While she and Sheldon initially share social cluelessness, after befriending Penny and Bernadette, she eventually becomes more interested in social and romantic interaction', ' Her relationship with Sheldon slowly progresses to the point at which Sheldon considers her his girlfriend, and eventually, they get married', ' Amy believes she and Penny are best friends, a sentiment that Penny does not initially share', ' Amy has a Ph', 'D', ' in neurobiology', ' Kevin Sussman as Stuart Bloom (recurring seasons 2â€“5, 7, starring seasons 6, 8â€“12):[18] A mild-mannered, under-confident owner of a comic book store', ' A competent artist, Stuart is a graduate of the prestigious Rhode Island School of Design', ' Though he is socially awkward, he possesses slightly better social skills', ' Stuart implies he is in financial trouble and that the comic book store now also is his home', \" He is later invited to join the guys' group while Howard is in space\", \" Stuart gets a new job caring for Howard's mother later\", ' After Mrs', \" Wolowitz's death, Stuart continues to live in her home, along with Howard and Bernadette, until he finds a place of his own\", ' Laura Spencer as Emily Sweeney (recurring seasons 7â€“8, 10, starring season 9):[19] A dermatologist at Huntington Hospital', ' Emily went to Harvard and delights in the macabre, and she states that she likes her job because she can cut things with knives', ' Prior to meeting Raj, Emily was set up on a blind date with Howard', \" After finding Emily's online dating profile, Raj has Amy contact her as his wingman instead\", \" Their relationship becomes exclusive, but Raj later breaks up with Emily when he becomes infatuated with Claire (Alessandra Torresani), a bartender and children's author\", ' Episodes Main article: List of The Big Bang Theory episodes Season\\tEpisodes\\tOriginally aired\\tViewers rank\\tU', 'S', ' Viewers (millions)\\t18â€“49 rank\\t18â€“49 rating/share First aired\\tLast aired 1 17\\tSeptember 24, 2007\\tMay 19, 2008\\t68[20]\\t8', '34\\t46[21]\\t3', '3/8 2 23\\tSeptember 22, 2008\\tMay 11, 2009\\t40[22]\\t10', '07\\tâ€”\\tâ€” 3 23\\tSeptember 21, 2009\\tMay 24, 2010\\t12\\t14', '22\\t5[23]\\t5', '3/13 4 24\\tSeptember 23, 2010\\tMay 19, 2011\\t13\\t13', '21\\t7[24]\\t4', '4/13 5 24\\tSeptember 22, 2011\\tMay 10, 2012\\t8\\t15', '82\\t6[25]\\t5', '5/17 6 24\\tSeptember 27, 2012\\tMay 16, 2013\\t3\\t18', '68\\t2[26]\\t6', '2/19 7 24\\tSeptember 26, 2013\\tMay 15, 2014\\t2\\t19', '96\\t2[27]\\t6', '2/20 8 24\\tSeptember 22, 2014\\tMay 7, 2015\\t2\\t19', '05\\t4[28]\\t5', '6/17 9 24\\tSeptember 21, 2015\\tMay 12, 2016\\t2\\t20', '36\\t3[29]\\t5', '8/19 10 24\\tSeptember 19, 2016\\tMay 11, 2017\\t2\\t18', '99\\t3[30]\\t4', '9/19 11 24\\tSeptember 25, 2017\\tMay 10, 2018\\t1\\t18', '63\\t5[31]\\t4', '4 12 24\\tSeptember 24, 2018\\tMay 16, 2019\\t2\\t17', '31\\t6[32]\\t3', \"6 Special May 16, 2019\\tâ€”\\tâ€”\\tâ€”\\tâ€” Production   Series' logo used for print The show's pilot episode premiered on September 24, 2007\", ' This was the second pilot produced for the show', ' A different pilot was produced for the 2006â€“07 television season but never aired', \" The structure of the original unaired pilot was different from the series' current form\", ' The only main characters retained in both pilots were Leonard (Johnny Galecki) and Sheldon (Jim Parsons), who are named after Sheldon Leonard, a longtime figure in episodic television as a producer, director, and actor', ' A minor character, Althea (Vernee Watson), appeared in the first scene of both pilots that was retained generally as-is', '[33] The first pilot included two female lead characters - Katie, \"a street-hardened, tough-as-nails woman with a vulnerable interior\" (played by Canadian actress Amanda Walsh),[34][35] and Gilda, a scientist colleague and friend of the male characters (played by Iris Bahr)', ' Sheldon and Leonard meet Katie after she breaks up with a boyfriend, and they invite her to share their apartment', \" Gilda is threatened by Katie's presence\", ' Test audiences reacted negatively to Katie, but they liked Sheldon and Leonard', ' The original pilot used Thomas Dolby\\'s hit \"She Blinded Me with Science\" as its theme song', '  Although the original pilot was not picked up, its creators were given an opportunity to retool it and produce a second pilot', ' They brought in the remaining cast and retooled the show to its final format', ' Katie was replaced by Penny (Kaley Cuoco)', ' The original unaired pilot has never been officially released, but it has circulated on the Internet', '[citation needed] On the evolution of the show, Chuck Lorre said, \"We did the \\'Big Bang Pilot\\' about two and a half years ago, and it sucked ', '', '', ' but there were two remarkable things that worked perfectly, and that was Johnny and Jim', ' We rewrote the thing entirely, and then we were blessed with Kaley and Simon and Kunal', '\" As to whether the world will ever see the original pilot on a future DVD release, Lorre said, \"Wow, that would be something', ' We will see', ' Show your failures', '', '', '\"[36]  The first and second pilots of The Big Bang Theory were directed by James Burrows, who did not continue with the show', ' The reworked second pilot led to a 13-episode order by CBS on May 14, 2007', '[37] Prior to its airing on CBS, the pilot episode was distributed on iTunes free of charge', ' The show premiered on September 24, 2007, and was picked up for a full 22-episode season on October 19, 2007', '[38] The show is filmed in front of a live audience,[39] and it is produced by Chuck Lorre Productions and Warner Bros', ' Television', '[40] Production was halted on November 6, 2007, due to the Writers Guild of America strike', ' Nearly three months later, on February 4, 2008, the series was temporarily replaced by a short-lived sitcom, Welcome to The Captain', ' The series returned on March 17, 2008, in an earlier time slot,[41] and ultimately only 17 episodes were produced for the first season', '[42][43]  After the strike ended, the show was picked up for a second season, airing in the 2008â€“2009 season, premiering in the same time slot on September 22, 2008', '[44] With increasing ratings, the show received a two-year renewal through the 2010â€“11 season in 2009', '[45][46] In 2011, the show was picked up for three more seasons', '[47] In March 2014, the show was renewed again for three more years through the 2016â€“17 season', ' This marked the second time the series gained a three-year renewal', '[48] In March 2017, the series was renewed for two additional seasons, bringing its total to 12, and running through the 2018â€“19 television season', \"[49]  Several of the actors on The Big Bang Theory previously worked together on the sitcom Roseanne, including Johnny Galecki, Sara Gilbert, Laurie Metcalf (who plays Sheldon's mother, Mary Cooper), and Meagen Fay (who plays Bernadette's mother)\", ' Additionally, Lorre was a writer on the series for several seasons', '  Science consultants David Saltzberg, a professor of physics and astronomy at the University of California, Los Angeles, checked scripts and provided dialogue, mathematics equations, and diagrams used as props', '[4] According to executive producer/cocreator Bill Prady, \"We\\'re working on giving Sheldon an actual problem that he\\'s going to be working on throughout the [first] season so there\\'s actual progress to the boards ', '', '', ' We worked hard to get all the science right', '\"[5] Saltzberg, who has a Ph', 'D', ' in physics, served as the science consultant for the show for six seasons and attended every taping', '[50] He saw early versions of scripts that needed scientific information added to them, and he also pointed out where the writers, despite their knowledge of science, had made a mistake', ' He was usually not needed during a taping unless a lot of science, and especially the whiteboard, was involved', '[51]  Saltzberg sometimes needed assistance on biology from Mayim Bialik, who has a Ph', 'D', ' in neuroscience', '[51]  Theme song  Single cover for \"Big Bang Theory Theme\" by Barenaked Ladies (2007) The Canadian alternative rock band Barenaked Ladies wrote and recorded the show\\'s theme song, which describes the history and formation of the universe and the Earth', \" Co-lead singer Ed Robertson was asked by Lorre and Prady to write a theme song for the show after the producers attended one of the band's concerts in Los Angeles\", \" Coincidentally, Robertson had recently read Simon Singh's book Big Bang,[52][53] and at the concert he improvised a freestyle rap about the origins of the universe\", '[citation needed] Lorre and Prady phoned him shortly thereafter and asked him to write the theme song', ' Having been asked to write songs for other films and shows, but ending up being rejected because producers favored songs by other artists, Robertson agreed to write the theme only after learning that Lorre and Prady had not asked anyone else', '[citation needed]  On October 9, 2007, a full-length (1 minute and 45 seconds) version of the song was released commercially', '[54] Although some unofficial pages identify the song title as \"History of Everything,\"[55] the cover art for the single identifies the title as \"Big Bang Theory Theme', '\" A music video also was released via special features on The Complete Fourth Season DVD and Blu-ray set', \"[56][57] The theme was included on the band's greatest hits album, Hits from Yesterday & the Day Before, released on September 27, 2011\", '[58] In September 2015, TMZ uncovered court documents showing that Steven Page sued[needs update] former bandmate Robertson over the song, alleging that he was promised 20 percent of the proceeds, but that Robertson has kept that money for himself', \"[59]  Actors' salaries For the first three seasons, Galecki, Parsons, and Cuoco, the three main stars of the show, received up to $60,000 per episode\", ' Their salaries rose to $200,000 per episode for the fourth season, then went up an additional $50,000 in each of the following three seasons, culminating in $350,000 per episode in the seventh season', '[60][61] In September 2013, Bialik and Rauch renegotiated the contracts they held since they were introduced to the series in 2010', ' On their old contracts, each was making $20,000â€“$30,000 per episode, while the new contracts doubled that, beginning at $60,000 per episode, increasing steadily to $100,000 per episode by the end of the contract, as well as adding another year for both', '[62]  By season seven, Galecki, Parsons, and Cuoco were also receiving 0', \"25 percent of the series' back-end money\", ' Before production began on the eighth season, the three plus Helberg and Nayyar looked to renegotiate new contracts, with Galecki, Parsons, and Cuoco seeking around $1 million per episode, as well as more back-end money', '[63] Contracts were signed in the beginning of August 2014, giving the three principal actors an estimated $1 million per episode for three years, with the possibility to extend for a fourth year', ' The deals also include larger pieces of the show, signing bonuses, production deals, and advances towards the back-end', '[64] Helberg and Nayyar were also able to renegotiate their contracts, giving them a per-episode pay in the \"mid-six-figure range\", up from around $100,000 per episode they each received in years prior', ' The duo, who were looking to have salary parity with Parsons, Galecki, and Cuoco, signed their contracts after the studio and producers threatened to write the characters out of the series if a deal could not be reached before the start of production on season eight', '[65] By season 10, Helberg and Nayyar reached the $1 million per episode parity with Galecki, Parsons, and Cuoco, due to a clause in their deals signed in 2014', '[66]  In March 2017, the main cast members (Galecki, Parsons, Cuoco, Helberg, and Nayyar) took a 10 percent pay cut to allow Bialik and Rauch an increase in their earnings', '[67] This put Galecki, Parsons, Cuoco, Helberg and Nayyar at $900,000 per episode, with Parsons, Galecki, and Helberg also receiving overall deals with Warner Bros', ' Television', '[49] By the end of April, Bialik and Rauch had signed deals to earn $500,000 per episode each, with the deals also including a separate development component for both actors', ' The deal was an increase from the $175,000â€“$200,000 the duo had been making per episode', '[68]  Recurring themes and elements Science Much of the series focuses on science, particularly physics', ' The four main male characters are employed at Caltech and have science-related occupations, as do Bernadette and Amy', ' The characters frequently banter about scientific theories or news (notably around the start of the show) and make science-related jokes', \"  Science has also interfered with the characters' romantic lives\", ' Leslie breaks up with Leonard when he sides with Sheldon in his support for string theory rather than loop quantum gravity', '[69] When Leonard joins Sheldon, Raj, and Howard on a three-month Arctic research trip, it separates Leonard and Penny at a time when their relationship is budding', \" When Bernadette takes an interest in Leonard's work, it makes both Penny and Howard envious and results in Howard confronting Leonard and Penny asking Sheldon to teach her physics\", '[70] Sheldon and Amy also briefly end their relationship after an argument over which of their fields is superior', '[71]  As the theme of the show revolves around science, many distinguished and high-profile scientists have appeared as guest stars on the show', ' Famous astrophysicist and Nobel laureate George Smoot had a cameo appearance in the second season', '[72] Chemical engineer and Nobel laureate Frances Arnold portrayed herself in the 12th season', '[73][74] Theoretical physicist Brian Greene appeared in the fourth season, as well as astrophysicist, science popularizer, and physics outreach specialist Neil deGrasse Tyson, who also appeared in the twelfth season', '[75] Cosmologist Stephen Hawking made a short guest appearance in a fifth-season episode;[76] in the eighth season, Hawking video conferences with Sheldon and Leonard, and he makes another appearance in the 200th episode', ' In the fifth and sixth seasons, NASA astronaut Michael J', \" Massimino played himself multiple times in the role of Howard's fellow astronaut\", ' Bill Nye appeared in the seventh and twelfth seasons', '[citation needed]  \"Nerd\" media  Star Trek: The Next Generation actor Wil Wheaton has a recurring role as a fictional version of himself on the show', ' The four main male characters are all avid fans of nerd culture', ' Among their shared interests are science fiction, fantasy, comic books, and collecting memorabilia', '  Star Trek in particular is frequently referenced, and Sheldon identifies strongly with the character of Spock, so much so that when he is given a used napkin signed by Leonard Nimoy as a Christmas gift from Penny, he is overwhelmed with excitement and gratitude (\"I possess the DNA of Leonard Nimoy?!\")', \"[77] Star Trek: The Original Series cast members William Shatner and George Takei have made cameos, and Leonard Nimoy made a cameo as the voice of Sheldon's vintage Mr\", ' Spock action figure', ' Star Trek: The Next Generation cast members Brent Spiner and LeVar Burton have had cameos as themselves,[78][79] while Wil Wheaton has a recurring role as a fictionalized version of himself', ' Leonard and Sheldon have had conversations in Klingon', '  They are also fans of Star Wars, Battlestar Galactica, and Doctor Who', ' James Earl Jones, Carrie Fisher and Mark Hamill make guest appearances', ' In the episode \"The Ornithophobia Diffusion\", when there is a delay in watching Star Wars on Blu-ray, Howard complains, \"If we don\\'t start soon, George Lucas is going to change it again\" (referring to Lucas\\' controversial alterations to the films)', ' In \"The Hot Troll Deviation\", Katee Sackhoff of Battlestar Galactica appeared as Howard\\'s fantasy dream girl', \" The characters have different tastes in franchises, with Sheldon praising Firefly but disapproving of Leonard's enjoyment of Babylon 5\", '[80] With regard to fantasy, the four make frequent references to The Lord of the Rings and Harry Potter novels and movies', ' Additionally, Howard can speak Sindarin, one of the two Elvish languages from The Lord of the Rings', '  Wednesday night is the group\\'s designated \"comic book night\"[81] because that is the day of the week when new comic books are released', ' The comic book store is run by fellow geek and recurring character Stuart', ' On a number of occasions, the group members have dressed up as pop culture characters, including The Flash, Aquaman, Frodo Baggins, Superman, Batman, Spock, The Doctor, Green Lantern, and Thor', '[82] As a consequence of losing a bet to Stuart and Wil Wheaton, the group members are forced to visit the comic book store dressed as Catwoman, Wonder Woman, Batgirl, and Supergirl', '[83] DC Comics announced that, to promote its comics, the company would sponsor Sheldon wearing Green Lantern T-shirts', '[84]  Various games have been featured, as well as referenced, on the series (e', 'g', ' World of Warcraft, Halo, Mario, Donkey Kong, etc', \"), including fictional games like Mystic Warlords of Ka'a (which became a reality in 2011)[85] and Rock-paper-scissors-lizard-Spock\", \"  Leonard and Penny's relationship One of the recurring plot lines is the relationship between Leonard and Penny\", ' Leonard becomes attracted to Penny in the pilot episode, and his need to do favors for her is a frequent point of humor in the first season', ' Meanwhile, Penny dates a series of muscular, attractive, unintelligent, and insensitive jocks', ' Their first long-term relationship begins when Leonard returns from a three-month expedition to the North Pole in the season 3 premiere', ' However, when Leonard tells Penny that he loves her, she realizes she cannot say it back, and they break up', \" Both Leonard and Penny go on to date other people, most notably with Leonard dating Raj's sister Priya for much of season 4\", ' This relationship is jeopardized when Leonard mistakenly comes to believe that Raj has slept with Penny, and it ultimately ends when Priya sleeps with a former boyfriend in \"The Good Guy Fluctuation\"', '  Penny, who admits to missing Leonard in \"The Roommate Transmogrification\", accepts his request to renew their relationship in \"The Beta Test Initiation\"', ' After Penny suggests having sex in \"The Launch Acceleration\", Leonard breaks the mood by proposing to her', ' Penny says \"no\" but does not break up with him', ' She stops a proposal a second time in \"The Tangible Affection Proof\"', ' In the sixth-season episode, \"The 43 Peculiarity\", Penny finally tells Leonard that she loves him', ' Although they both feel jealousy when the other receives significant attention from the opposite sex, Penny is secure in their relationship, even when he leaves on a four-month expedition to the North Sea in \"The Bon Voyage Reaction\"', ' After he returns, the relationship blossoms over the seventh season', ' In the penultimate episode \"The Gorilla Dissolution\", Penny admits that they should marry and when Leonard realizes that she is serious, he proposes with a ring that he has been carrying for years', ' Leonard and Penny decide to elope to Las Vegas in the season 8 finale, but beforehand, wanting no secrets, Leonard admits to kissing another woman, Mandy Chow (Melissa Tang) while on the expedition', ' Despite this, Leonard and Penny finally marry in the season 9 premiere and remain happy', ' By the Season 9 finale, Penny and Leonard decide to have a second wedding ceremony for their family and friends, to make up for eloping', \" In season 10, Sheldon moves into Penny's old apartment with Amy, allowing Penny and Leonard to finally live on their own as husband and wife\", '  In season 12, Penny announces that she does not want to have any children and Leonard reluctantly supports her decision', ' Later, her old boyfriend Zack and his new wife want Leonard to be a surrogate father to their kid since Zack is infertile', ' Penny reluctantly agrees to let Leonard donate his sperm', ' However, when she tries to seduce Leonard despite knowing he has to be abstinent for a few days, her visiting father, Wyatt, points out to Penny that her own actions suggest she is more conflicted over having kids than she lets on, and she admits she feels bad about letting him and Leonard down if she never has children', ' He says that despite her flaws, parenthood is the best thing that ever happened to him, and he does not want her to miss out, but that he will support her no matter what she does', ' Leonard eventually changes his mind, not wanting a child in the world that he cannot raise', \" In the series finale, Penny is pregnant with Leonard's baby, and she has changed her mind about not wanting children\", \"  Sheldon and Amy's relationship In the third-season finale, Raj and Howard sign Sheldon up for online dating to find a woman compatible with Sheldon, and they discover neurobiologist Amy Farrah Fowler\", ' Like Sheldon, she has a history of social ineptitude and participates in online dating only to fulfill an agreement with her mother', ' This spawns a story line in which Sheldon and Amy communicate daily while insisting to Leonard and Penny that they are not romantically involved', ' In \"The Agreement Dissection\", Sheldon and Amy talk in her apartment after a night of dancing, and she kisses him on the lips', ' Instead of getting annoyed, Sheldon says \"fascinating\" and later asks Amy to be his girlfriend in \"The Flaming Spittoon Acquisition\"', ' The same night he draws up \"The Relationship Agreement\" to verify the ground rules of him as her boyfriend and vice versa (similar to his \"Roommate Agreement\" with Leonard)', ' Amy agrees but later regrets not having had a lawyer read through it', '  In \"The Launch Acceleration\", Amy tries to use her \"neurobiology bag of tricks\" to increase the attraction between herself and Sheldon', ' Her efforts appear to be working because Sheldon is not happy, but he makes no attempt to stop her', ' In the fifth-season finale, \"The Countdown Reflection\", Sheldon takes Amy\\'s hand as Howard is launched into space', ' In the sixth-season premiere, \"The Date Night Variable\", after a dinner in which Sheldon fails to live up to this expectation, Amy gives Sheldon an ultimatum that their relationship is over unless he tells her something from his heart', \" Amy accepts Sheldon's romantic speech even after learning that it is a line from the first Spider-Man movie\", ' In \"The Cooper/Kripke Inversion\", Sheldon states that he has been working on his discomfort about physical contact and admits that \"it\\'s a possibility\" that he could one day have sex with Amy', ' Amy is revealed to have similar feelings in \"The Love Spell Potential\"', ' Sheldon explains that he never thought about intimacy with anyone before Amy', '[86]  \"The Locomotive Manipulation\" is the first episode in which Sheldon initiates a kiss with Amy', ' Although initially done in a fit of sarcasm, he discovers that he enjoys the feeling', ' Consequently, Sheldon slowly starts to open up over the rest of the season, and he starts a more intimate relationship with Amy', ' However, in the season finale, Sheldon leaves town temporarily to cope with several changes and Amy becomes distraught', ' However, 45 days into the trip, Sheldon gets mugged and calls for Leonard to drive him home, only to be confronted by Amy, who is upset over not being contacted by him in weeks', ' When Sheldon admits he did not call her because he was too embarrassed to admit that he could not make it on his own, Amy accepts that he is not perfect', ' In \"The Prom Equivalency\", Sheldon hides in his room to avoid going to a mock prom reenactment with her', ' In the resulting standoff, Amy is about to confess that she loves Sheldon, but he surprises her by saying that he loves her too', ' This prompts Amy to have a panic attack', '  In the season-eight finale, Sheldon and Amy get into a fight about commitment on their fifth anniversary', ' Amy tells Sheldon that she needs to think about the future of their relationship, unaware that Sheldon was about to propose to her', ' Season nine sees Sheldon harassing Amy about making up her mind until she breaks up with him', \" Both struggle with singlehood and trying to be friends for the next few weeks until they reunite in episode ten and have sex for the first time on Amy's birthday\", \" In season ten, Amy's apartment is flooded, and she and Sheldon decide to move in together into Penny's apartment as part of a five-week experiment to determine compatibility with each other's living habits\", ' It goes well and they decide to make the arrangement permanent', '  In the Season 11 premiere, Sheldon proposes to Amy, and she accepts', ' The two get married in the eleventh-season finale', '  \"Soft Kitty\" Main article: Soft Kitty The song \"Soft Kitty\" is described by Sheldon as a song sung by his mother when he was ill', ' Its repeated use in the series popularized the song', \"[87] A scene showing the origin of the song in Sheldon's childhood is depicted in an episode of Young Sheldon, which aired on February 1, 2018\", \" It shows Sheldon's mother, Mary, singing the song to her son, who has the flu\", \"[88]  Howard's mother In scenes set at Howard's home, he interacts with his rarely seen mother (voiced by Carol Ann Susi until her death) by shouting from room to room in the house\", ' She similarly interacts with other characters in this manner', \"[89] She reflects the Jewish mother stereotype in some ways, such as being overly controlling of Howard's adult life and sometimes trying to make him feel guilty about causing her trouble\", ' She is dependent on Howard, as she requires him to help her with her wig and makeup in the morning', ' Howard, in turn, is attached to his mother to the point where she still cuts his meat for him, takes him to the dentist, does his laundry and \"grounds\" him when he returns home after briefly moving out', \"[90] Until Howard's marriage to Bernadette in the fifth-season finale, Howard's former living situation led Leonard's psychiatrist mother to speculate that he may suffer from some type of pathology[91] and Sheldon to refer to their relationship as Oedipal\", \"[92] In season 8, Howard's mother dies in her sleep while in Florida, which devastates Howard and Stuart, who briefly lived with Mrs\", ' Wolowitz', '  Apartment building elevator In the apartment building where Sheldon, Leonard, and Penny (and later Amy) live, the elevator has been out of order throughout most of the series, forcing characters to have to use the stairs', ' Stairway conversations between characters occur in almost every episode, often serving as a transition between longer scenes', ' The Season 3 episode, \"The Staircase Implementation\" reveals that the elevator was broken when Leonard was experimenting with rocket fuel', '[93] In the penultimate episode of the series, the elevator is returned to an operational state, causing Sheldon some angst, until he realizes that the fixed elevator reverted things to the \"status quo\"', '  Vanity cards Like most shows created by Chuck Lorre, The Big Bang Theory ends by showing for one second a vanity card written by Lorre after the credits, followed by the Warner Bros', ' Television closing logo', \" These cards are archived on Lorre's website\", '[94] The series\\' final vanity card reads simply \"The End\"', '[95]  Release Broadcast The Big Bang Theory premiered in the United States on September 24, 2007, on CBS', ' The series debuted in Canada on CTV in September 2007', '[96] On February 14, 2008, the series debuted in the United Kingdom on channels E4 and Channel 4', '[97] In Australia the first seven seasons of the series began airing on the Seven Network and 7mate from October 2015 and also gained the rights to season 8 in 2016, although the Nine Network has rights to air seasons nine & ten', '[98][99] On January 22, 2018, it was announced that Nine had acquired the rights to Season 1â€“8', \"[100]  Syndication In May 2010, it was reported that the show had been picked up for syndication, mainly among Fox's owned and operated stations and other local stations, with Warner Bros\", \" Television's sister cable network TBS holding the show's cable syndication rights\", ' Broadcast of old episodes began airing in September 2011', \" TBS now airs the series in primetime on Tuesdays, Wednesdays, and Thursdays, with evening broadcasts on Saturdays (TBS's former local sister station in Atlanta also holds local weeknight rights to the series)\", '[101] Although details of the syndication deal have not been revealed, it was reported the deal \"set a record price for a cable off-network sitcom purchase\"', '[102] CTV holds national broadcast syndication rights in Canada, while sister cable network The Comedy Network holds cable rights', '  Online media Warner Bros', ' Television controls the online rights for the show', '[103][104] Full episodes were available at tv', 'com, while short clips and recently aired full episodes were available on cbs', 'com and later in its run on CBS All Access', '[105] In Canada, recent episode(s) and pictures are available on CTV', 'ca', \"[106] Additionally in Canada, the first six seasons are available for streaming on Bell Media's CraveTV\", \"[107] After the show has aired in New Zealand the shows are available in full online at TVNZ's on demand web service\", '  In 2020, the show became available in the United States on HBO Max', '[108]  Home media Name\\tNo', ' of episodes\\tRelease dates Region 1\\tRegion 2\\tRegion 4 The Complete First Season\\t17\\tSeptember 2, 2008[109]\\tJanuary 12, 2009[110]\\tApril 3, 2009[111] The Complete Second Season\\t23\\tSeptember 15, 2009[112]\\tOctober 19, 2009[113]\\tMarch 3, 2010[114] The Complete Third Season\\t23\\tSeptember 14, 2010[115]\\tSeptember 27, 2010[116]\\tOctober 13, 2010[117] The Complete Fourth Season\\t24\\tSeptember 13, 2011[118]\\tSeptember 26, 2011[119]\\tOctober 5, 2011[120] The Complete Fifth Season\\t24\\tSeptember 11, 2012[121]\\tSeptember 3, 2012[122]\\tOctober 3, 2012[123] The Complete Sixth Season\\t24\\tSeptember 10, 2013[124]\\tSeptember 2, 2013[125]\\tOctober 11, 2013[126] The Complete Seventh Season\\t24\\tSeptember 16, 2014[127]\\tSeptember 8, 2014[128]\\tSeptember 17, 2014[129] The Complete Eighth Season\\t24\\tSeptember 15, 2015[130]\\tSeptember 14, 2015[131]\\tSeptember 16, 2015[132] The Complete Ninth Season\\t24\\tSeptember 13, 2016[133]\\tAugust 29, 2016[134]\\tAugust 31, 2016[135] The Complete Tenth Season\\t24\\tSeptember 12, 2017[136]\\tSeptember 11, 2017[137]\\tSeptember 13, 2017[138] The Complete Eleventh Season\\t24\\tSeptember 11, 2018[139]\\tSeptember 24, 2018[140]\\tSeptember 12, 2018[141] The Twelfth and Final Season\\t24\\tNovember 12, 2019[142]\\tNovember 11, 2019[143]\\tNovember 13, 2019[144] The Complete Series\\t279\\tNovember 12, 2019[145]\\tNovember 11, 2019[146]\\tNovember 13, 2019[147] The first and second seasons were only available on DVD at their time of release in 2008[148] and 2009', '[149] Starting with the release of the third season in 2010[150] and continuing every year with every new season, a Blu-ray disc set has also been released in conjunction with the DVD', ' In 2012, Warner Bros', ' released the first two seasons on Blu-ray,[151] marking the first time that all episodes were available on the Blu-ray disc format', ' ']\n"
     ]
    }
   ],
   "source": [
    "my_file = open(\"tbbt_wiki.txt\", \"r\") \n",
    "  \n",
    "# reading the file containg information\n",
    "data = my_file.read()\n",
    "#convert data into list\n",
    "data_into_list = data.replace('\\n', ' ').split(\".\") \n",
    "\n",
    "# printing the data \n",
    "print(data_into_list) \n",
    "my_file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabs: {'fails', '26', 'eleventh', 'returns', 'The', 'show', 'geeky', 'which', 'whether', 'doctorate', 'evolution', '19', 'five-week', 'Batgirl', 'hold', 'slot', 'Greene', 'pages', 'because', 'Stuart', 'wedding', 'Her', 'night', 'expectation', 'calls', 'income', 'microbiologist', 'record', 'version', 'Howard', 'Laurie', 'romantic', '4.4', 'supporting', '62', '15', '128', 'progresses', '74', 'provide', 'Mr.', 'intimate', 'An', 'marked', 'America', '24', 'sung', 'Nye', 'company', 'grasp', 'fifth', 'if', 'ground', 'high-profile', 'ceremony', 'ranked', 'fascinating', 'fuel', 'CraveTV', 'their', 'artists', '42', 'order', 'School', 'Vegas', 'herself', 'ever', 'Pilot', 'Among', 'led', 'William', 'New', 'universe', 'named', 'Oedipal', 'unofficial', '40', 'laureate', 'movie', 'assistance', 'memory', 'oddly', 'rest', 'young', 'does', 'book', 'Doctor', 'home', 'cut', 'eleven', 'Hamill', 'premiering', 'something', 'possesses', 'dialogue', 'controversial', 'between', 'neuroscience', 'via', '14.22', 'Comedy', 'premiere', 'Jewish', 'Frodo', 'toys', 'material', 'comic', '139', 'figure', 'concert', 'Final', '76', 'Ann', 'still', 'world', 'IQ', 'seven', 'introduced', 'bringing', 'need', 'banter', 'Cinnamon', 'shortly', 'development', 'days', 'interests', 'discovers', 'Flash', 'finding', 'determined', 'aerospace', 'dating', 'role', 'visit', 'ago', \"''\", 'primarily', 'state', 'references', 'help', '2008–2009', 'adding', '107', 'neurobiology', 'agrees', 'TVNZ', 'Babylon', 'mock', 'Day', 'Fox', 'Chow', 'mechanics', 'Caltech', 'Troll', 'straight', 'Arctic', 'Last', '119', '125', 'marking', 'astronaut', 'mood', 'Rauch', 'national', '!', 'Best', 'original', 'Singh', 'filmed', 'science-related', 'date', 'biology', '22', '41', 'unable', 'infatuated', 'old', 'briefly', 'reverted', 'Starting', 'April', 'she', 'Awards', 'appear', 'By', 'lot', 'Texas', 'time', 'full-length', 'January', 'shared', '115', 'cope', 'hope', 'better', 'Simon', 'Spencer', 'sperm', '73', 'price', 'range', 'Max', 'friends', 'Penny', 'September', 'replaced', 'run', 'some', 'initially', 'On', 'science', 'Mark', 'hides', 'share', 'used', '79', 'Amanda', '``', 'bar', 'meeting', 'controlling', 'released', 'Helberg', '200th', 'actress', 'singlehood', 'sitcom', 'Massachusetts', 'hang', 'Comics', '.', '200,000', 'Star', 'Watson', 'million', 'Wyatt', 'possess', 'Various', 'slowly', 'Indian', 'envious', 'sarcasm', 'local', 'describes', 'broken', 'Complete', 'longer', 'a', 'interacts', '900,000', 'physics', 'influence', 'identifies', '60', 'without', 'Several', 'accepts', 'intimacy', 'History', 'remarkable', 'Additionally', 'within', 'was', 'from', 'prequel', 'Before', 'writer', 'reads', 'Neil', 'arrangement', 'fictional', '9', 'getting', 'ill.', 'admits', 'weeknight', ')', 'Me', 'retooled', 'Gilbert', 'trouble', 'progress', 'primetime', 'I', 'living', 'harassing', 'If', 'extremely', 'feelings', 'reflects', 'One', 'profile', 'whom', 'half', 'point', '2011', '141', 'narrating', 'for', 'World', 'suggests', 'mid-six-figure', 'rules', 'February', 'simply', 'Pole', 'not', 'conversations', 'proceeds', 'allowing', 'boasts', 'rose', 'Lead', 'structure', 'next', 'Galecki', 'Space', 'morning', \"'s\", 'DVD', 'secure', 'complains', 'we', 'believe', 'closing', 'portrayed', 'signed', 'NASA', 'microbiology', 'goes', 'marry', 'Dragons', 'Wolowitz', 'can', 'rating/share', 'dream', 'Harvard', 'efforts', 'engagement', 'Deviation', 'more', ',', 'Until', 'thereafter', 'Superman', 'Ka', 'feel', '137', 'Tangible', 'stereotypical', 'happy', 'Globe', 'missing', 'fulfill', 'Tang', 'Channel', 'mind', 'boyfriend', 'author', 'Roommate', 'Full', '72', 'Potter', 'Bloom', 'frequently', 'Sweeney', '120', \"n't\", 'origins', 'college', 'research', 'propose', 'break', 'tricks', 'on', 'citation', '12th', 'head', 'ladies', 'quo', 'lived', 'popularizer', 'saw', 'Atlanta', '187', 'invite', '12', 'Guild', '5.3/13', 'pay', 'Hofstadter', 'gives', 'Rock-paper-scissors-lizard-Spock', 'going', 'finds', 'Firefly', 'delay', 'tv.com', 'mugged', 'books', 'starts', 'web', 'insensitive', 'work', 'University', 'Wars', 'fifth-season', 'included', 'starring', 'routine', 'making', 'different', 'written', 'remove', 'reenactment', 'jocks', 'forcing', 'sleeps', 'vindictive', 'or', 'its', 'All', 'alcohol', 'Single', 'Roseanne', 'scientists', 'executive', 'learning', '106', 'sixteen', 'Rings', 'nine', 'standoff', 'annoyed', '16', 'shouting', '134', 'particular', '131', 'there', 'We', 'up', 'across', 'astrophysicist', 'shows', 'pieces', 'favored', 'operational', '2016–17', 'by', 'sharing', 'ratings', '18.99', 'woman', 'Bahr', 'Captain', 'Cosmologist', 'kept', '109', 'total', 'Mary', '8–12', 'Las', 'under-confident', 'Brent', 'Fifth', 'Internet', 'this', 'They', 'purchase', 'unaired', 'participates', 'Ninth', 'plus', 'socially', 'August', 'became', 'under', 'ways', '2019', 'pregnant', 'asks', 'biochemist', 'guest', 'donate', 'type', 'Winkle', 'pathology', 'streaming', 'may', 'end', 'them', 'usually', 'asking', 'married', 'third-season', 'cameos', 'regard', 'essentially', 'occur', 'lips', 'early', 'Lucy', 'feminine', 'video', 'use', 'producer', 'When', 'beginning', 'eight', 'Rhode', 'prodigy', 'got', 'generally', 'short', 'Release', 'skills', '89', 'Factory', 'airing', 'Test', '18', 'of', 'watching', 'Welcome', '6', 'Galactica', 'son', 'later', 'Over', 'Shatner', '23', 'States', '18.63', 'Huntington', 'principal', 'Florida', 'Viewers', 'template', 'Sea', 'lash', 'into', 'He', 'movies', 'fixed', 'Los', 'strike', 'guys', 'long-term', 'Countdown', 'friend', 'occasionally', 'ring', 'Rostenkowski-Wolowitz', 'payload', 'Cast', '4', 'Manipulation', '104', 'becomes', 'dancing', '83', 'component', 'elements', 'languages', 'reviews', 'third', 'saying', 'fellow', 'Claire', 'scripts', 'quirks', 'deal', 'both', 'almost', 'seventh', 'T-shirts', 'realizes', 'pilot', 'hotel', 'recorded', 'Sindarin', 'Unlike', '87', 'commercially', 'Implementation', 'Originally', 'astronomy', 'Dissection', '103', '(', 'worked', 'Burrows', 'halted', 'Wil', 'life', 'becoming', 'requires', 'panic', 'reunite', 'dependent', 'ends', 'write', 'fit', 'muscular', 'Tenth', 'contacted', 'Wheaton', 'bartender', 'minute', 'Angeles', 'reported', '108', 'eventually', '148', 'HBO', 'request', 'consequence', 'professor', 'one', '20.36', '91', '[', 'wife', 'suffer', 'he', 'Later', 'see', 'owned', 'Lord', '66', 'were', 'childhood', 'works', 'Fay', 'voiced', 'online', 'promised', 'sued', 'reacted', '86', 'began', '65', 'Molaro', 'neighbor', 'members', 'Nimoy', 'Online', 'opportunity', 'studying', 'teach', 'increasing', 'school', 'speculate', '2008', 'money', 'aspects', 'embarrassed', 'Page', 'North', 'Dolby', 'served', 'let', 'house', '129', '7mate', 'unaware', 'Dungeons', 'Rostenkowski', 'throughout', 'experimental', 'distinguished', 'job', 'flooded', '6.2/19', 'syndication', '101', 'Name', 'aired', 'agreement', 'Ed', 'Terrier', 'announces', 'happened', 'towards', 'news', 'Theoretical', 'talk', 'sister', 'slightly', 'Everything', 'overwhelmed', 'drive', '143', '116', 'single', 'attractive', 'Theory', 'Special', 'among', 'received', 'Generation', 'rewrote', 'ended', 'studio', 'resulting', 'decide', 'final', 'Bernadette', 'cover', '53', 'involved', 'main', 'airs', '112', '70', 'jeopardized', 'alternative', '29', 'held', 'devastates', 'gets', 'find', 'scene', 'attached', 'proposes', 'aspiring', 'able', 'difficult', 'Wednesday', 'cameo', 'member', 'Spittoon', 'Metcalf', 'string', 'be', '17.31', 'star', '14', 'Golden', 'debuted', 'theoretical', 'For', '—', 'Glendale', 'television', '31', 'Hospital', '122', 'holding', 'iTunes', '5.5/17', '93', 'lab', 'avid', 'apartment', 'apartments', 'how', 'opposite', 'details', 'conflicted', 'article', 'six', '133', 'alterations', 'forced', 'Variable', 'put', 'Production', 'feels', 'but', 'prom', 'tastes', 'Main', 'no', 'Yorkshire', 'Launch', 'TBS', '4.9/19', 'three-year', 'running', 'current', '82', 'Green', 'former', '150', 'family', 'Twelfth', 'CBS', 'alleging', 'than', 'to', '3.3/8', 'Katee', 'season', '113', '27', 'based', '15.82', 'percent', '?', 'fight', 'Apartment', 'relationship', 'Bialik', 'surrogate', 'element', 'Nerd', 'renew', 'interfered', 'moving', 'mother', 'employed', 'finale', 'March', 'culture', 'featured', 'social', 'coworkers', 'during', 'years', 'rights', 'went', 'Hot', 'Rajesh', 'out', 'tendency', 'Much', 'history', '50', '69', 'actions', 'Alessandra', 'knowledge', 'engineer', 'Spiner', '84', 'discomfort', 'likes', 'blind', 'Lucas', 'argues', 'cable', 'macabre', 'producers', 'perfectly', 'hall', 'never', 'Eleventh', 'female', 'nerd', 'perspective', 'Soft', '33', '92', 'resumed', 'diagrams', 'anyone', 'mainly', 'jokes', 'checked', 'superior', '1–8', 'cast', 'Katie', 'promote', 'now', 'Takei', 'Meagen', 'repeated', 'Equivalency', 'evening', 'Farrah', 'Though', 'Bon', 'Learn', '46', 'A', 'Nebraska', 'Susi', '...', 'Spell', 'dentist', 'before', 'takes', 'sides', 'rap', 'Beta', 'Chuck', 'Potential', 'apparently', 'took', 'states', 'Yesterday', 'loves', 'Wonder', 'determine', 'Co-lead', 'had', 'caring', 'future', 'CTV', 'sometimes', 'actors', '81', 'flu', 'after', 'Transmogrification', 'reception', 'age', 'mathematics', 'slept', 'Mayim', 'well', 'theory', 'row', '32', '5.8/19', 'Young', 'production', 'sees', 'Warner', 'previously', 'losing', 'Bros', 'attended', '2013', 'Big', 'franchises', 'popularized', 'stairs', 'singer', 'United', 'also', 'weeks', 'Hawking', '132', 'per', 'stations', 'that', 'broadcasts', 'reworked', 'pictures', '124', '175,000–', 'scenes', 'Region', 'Halo', 'fields', 'CA', '98', 'features', 'reached', 'theories', 'situations', 'speak', 'concierge', 'showing', 'Carrie', 'Carol', 'left', 'song', 'engaged', 'grounds', 'sweet', 'Episodes', 'occasions', 'Wow', 'phoned', 'situation', 'Season', 'baby', '&', 'clearly', 'nothing', 'May', 'Nayyar', 'needed', 'budding', 'Lorre', 'contact', 'story', 'done', 'Kate', 'possibility', 'music', 'American', 'additional', 'have', 'vanity', 'believes', 'remain', 'joins', '30,000', 'renegotiated', 'spot', 'provided', 'waitress', 'your', 'Peculiarity', 'nominations', 'leaves', 'favorable', '0.25', 'top', 'Proof', 'two-year', 'confronting', '127', 'knowing', 'lawyer', '63', 'when', 'compatible', 'threatened', 'confess', 'Barenaked', 'card', 'romantically', 'miss', 'Network', 'stops', 'bonuses', '2018', 'months', 'in-universe', 'concerts', 'Ornithophobia', 'archived', 'is', 'admit', 'controls', 'initiates', '21', 'renewed', 'unintelligent', 'Gorilla', 'Canada', 'anxiety', 'master', 'album', 'agreed', 'season-eight', 'Mrs.', 'sleep', 'recurring', 'retained', 'hits', 'like', 'rank', 'quantum', 'times', 'friendship', '61', 'counterpart', 'seconds', 'Bell', 'played', '54', 'things', 'girl', 'soon', 'infertile', 'room', 'Cuoco', 'changes', 'service', 'over', 'referring', 'who', 'actor', 'on-again', 'renegotiate', 'Famous', 'As', 'gift', '1', 'waste', '55', 'move', 'artist', '34', 'theme', 'release', 'broadcast', 'collecting', 'three-month', 'death', 'what', 'Bros.', 'films', 'special', '2010–11', '47', 'town', 'wanting', 'occupations', 'breaks', 'gratitude', 'voice', 'increase', 'suffers', 'befriending', 'appearance', 'Staircase', 'Saturdays', 'said', 'End', 'Sara', 'Batman', 'allow', 'wearing', 'Harry', 'Vanity', 'fictionalized', 'earnings', 'commitment', 'won', 'logo', '500,000', 'per-episode', 'section', 'representative', 'appearances', '350,000', '2014', '130', 'gravity', 'lets', 'parenthood', '3.6', 'With', 'deGrasse', 'often', '35', 'rejected', 'Theme', 'meat', 'outreach', 'Parsons', \"'\", 'Zack', 'way', 'Mandy', 'vintage', 'distributed', 'information', 'equations', 'Locomotive', 'speaks', '88', 'Baggins', 'character', 'instead', 'Stairway', 'expedition', 'Jones', 'other', 'similarly', 'seen', 'street-hardened', 'neurobiologist', 'common', 'directed', 'Initially', 'could', 'researching', 'roles', 'much', 'enjoys', 'prompts', 'Warcraft', 'e.g', 'November', 'until', '151', 'group', '2018–19', 'knives', 'receives', 'attention', 'although', 'Station', 'Iris', 'Debbie', 'people', 'blossoms', 'attack', '18–49', 'holds', 'earn', 'Diffusion', 'liked', '3', 'mutism', 'fancies', 'games', '36', 'etc', 'daily', ';', 'sex', 'depicted', 'salaries', 'tells', '77', 'Sackhoff', 'form', 'Like', 'Media', 'do', 'verify', '18.68', 'designated', 'mistake', 'Saltzberg', 'hand', 'CTV.ca', 'Nearly', 'many', \"'re\", '48', 'needs', 'argument', 'husband', 'Actors', 'ineptitude', 'looked', 'comes', 'Nobel', 'Aquaman', 'rarely', 'Donkey', 'Spock', 'thing', 'novels', 'matter', '97', 'has', '20,000–', 'songs', 'extend', 'George', 'open', 'loop', 'episode', 'Science', '144', 'continue', 'upset', 'since', 'think', 'these', '2010', 'officially', 'habits', 'conjunction', 'change', 'where', 'promoted', 'meet', 'circulated', 'discover', 'would', 'following', '67', 'provoked', 'While', 'series', 'taping', 'off-network', 'made', 'Broadcast', '118', 'Recurring', 'join', 'rewrite', \"'Big\", 'selected', 'refer', 'message', '102', 'off-again', 'rock', 'millions', 'According', 'person', 'girlfriend', 'low', 'him', 'failures', 'week', 'Their', 'place', '2–5', 'elevator', 'memorabilia', 'U.S.', 'Affection', 'laundry', 'Leslie', 'station', 'very', 'Although', '49', 'Outstanding', 'first', 'site', 'List', 'Delhi', 'exclusive', 'colleague', '56', 'Cheesecake', 'Guy', 'Productions', 'decision', 'fuse', '44', 'tries', 'DC', 'cbs.com', 'Blu-ray', 'larger', 'continually', 'contract', '13-episode', 'particle', 'the', 'abstinent', '117', 'causing', 'Raj', 'stop', 'Robertson', 'stereotype', 'Koothrappali', 'Brian', 'geek', '173', 'free', 'intelligence', 'blessed', 'particularly', '71', 'sixth', 'picked', 'carrying', '2', '4.4/13', 'read', 'mate', 'competent', 'guilty', 'After', 'centered', 'unless', 'launched', 'secrets', 'negatively', 'get', 'Ph.D.', '-', 'separate', 'continues', 'eidetic', 'in', 'given', 'doubled', 'Show', 'Australia', 'Wednesdays', '90', 'ultimately', '43', 'temporarily', 'physical', 'Walsh', 'awkward', '2006–07', 'pharmaceutical', '80', 'wingman', 'sponsor', '111', '20', 'seeking', 'Home', '100', 'being', 'Lantern', 'mistakenly', 'cards', 'co-worker', 'non-fictional', 'Earl', 'say', 'dressed', 'trying', 'fans', 'Meanwhile', ':', 'art', 'it', 'beforehand', 'Another', 'uncovered', '136', 'through', 'Having', 'enjoyment', 'Actor', 'about', 'director', 'Its', 'themes', 'explain', 'David', 'returned', 'dermatologist', 'prestigious', 'Agreement', 'back', 'conferences', 'due', 'his', 'strongly', 'followed', 'interaction', 'psychiatrist', 'It', '45', 'struggle', 'parity', 'renewal', 'receiving', 'This', 'Fisher', 'Prior', 'Prady', 'tables', 'comics', 'stars', 'entirely', 'did', '145', 'described', 'regrets', 'changed', 'implies', '78', 'humor', 'props', 'action', 'short-lived', 'Gupta', 'they', 'Stephen', 'delights', 'versions', 'credits', 'most', 'Amy', 'Relationship', 'referenced', 'Smoot', 'Technology', 'vice', 'ending', 'Althea', 'pilots', '149', '7–8', 'bad', 'style', 'fourth', 'asked', 'sentiment', 'remaining', 'rendering', '114', 'as-is', 'Zealand', 'Jim', 'fantasy', 'perfect', 'working', 'Nine', 'episodic', 'same', '64', '2009', 'are', 'around', 'spinoff', 'Kevin', 'Rati', 'Contracts', 'Cooper/Kripke', 'demand', 'Melissa', 'communicate', 'Third', 'kisses', 'scientist', 'Eighth', '2020', 'charge', 'giving', '19.05', 'advances', 'Elvish', 'birthday', 'manner', 'full', 'reality', 'Supergirl', 'successful', 'casual', 'second', 'praising', '28', 'Christmas', 'front', 'elope', 'good-natured', 'lines', 'duo', '10', 'despite', 'Woman', 'Mario', 'Bill', 'court', 'graduate', 'feeling', 'First', 'Fowler', 'at', 'napkin', 'building', 'ritualized', 'points', 'actual', 'network', 'Flaming', 'makes', 'each', 'totaling', 'overall', 'conflicting', 'Both', 'recent', 'live', 'kids', '51', '85', 'Priya', 'own', 'sign', '8', 'Writers', 'Good', 'Kingdom', 'update', 'Sussman', 'neuroscientist', 'serving', 'Kunal', 'new', '58', 'Canadian', 'steadily', '25', 'three', 'distraught', '22-episode', 'four-month', 'any', 'Sixth', 'status', 'kid', 'plot', 'Dissolution', 'as', '5', 'tough-as-nails', 'frequent', 'improvised', 'proposing', 'Consequently', 'should', 'ten', 'make', 'India', 'with', 'surprises', 'episodes', 'origin', 'pointed', 'plays', 'best', 'Kong', 'print', 'even', 'right', '17', 'another', 'compatibility', 'signing', 'attraction', 'will', 'sixth-season', 'earned', '126', 'brought', '37', 'every', 'Inversion', 'eleventh-season', 'transition', 'Micucci', 'day', '68', 'Galveston', '105', 'eighth', '2016', 'Leonard', 'five', '10.07', '60,000', 'Thor', '138', 'penultimate', 'Vernee', 'looking', 'These', 'financial', 'else', 'Series', 'Anu', 'her', 'Date', 'results', 'Gilda', 'Syndication', 'child', '13', 'few', 'insisting', '39', 'Initiation', 'produce', 'Tyson', 'kiss', 'so', 'only', 'presence', 'two', 'supports', 'similar', 'too', 'want', '38', 'excitement', 'anniversary', 'Coincidentally', 'kissing', 'Earth', 'set', 'call', 'rather', 'again', 'experimenting', 'number', '94', 'She', 'Massimino', 'hard', 'go', 'year', 'high', 'focuses', 'Television', 'come', 'producer/cocreator', 'scientific', 'visiting', 'considers', 'Cooper', 'Institute', 'Voyage', 'Johnny', '5.6/17', 'disapproving', 'contracts', '13.21', '4–12', 'wrote', 'Steven', 'E4', 'Laura', '8.34', 'rocket', '142', 'pop', 'consultant', 'LeVar', 'operated', 'Mystic', 'problem', 'identify', 'No', 'continuing', 'draws', 'Omaha', 'Design', 'interior', 'Seventh', 'multiple', 'Trek', 'TMZ', 'channels', '57', 'Love', 'recently', 'thought', 'himself', 'documents', 'support', 'However', 'male', 'overcoming', 'notably', 'Prom', 'part', 'disc', 'deals', 'Kitty', 'Ladies', 'revolves', 'serious', 'Who', 'Michael', 'Chemical', 'salary', 'permanent', 'down', 'earlier', 'trains', 'especially', 'enough', 'dinner', 'dies', '2012', 'suggest', 'angst', 'estimated', 'such', 'selective', 'Night', 'speech', 'appeared', 'bag', 'available', 'spawns', 'makeup', '99', 'an', 'Please', 'twelfth', 'Seven', 'father', 'band', ']', 'degree', 'nominated', 'DNA', '2017', 'cuts', 'produced', 'seasons', 'having', 'singing', 'Frances', 'attempt', '$', 'man', '19.96', 'raise', 'air', 'owner', 'and', 'marriage', 'says', 'wig', 'added', 'Island', 'characters', 'finally', 'James', 'letting', 'line', 'Emily', 'space', 'flaws', 'Second', 'reveals', 'seduce', 'eloping', 'format', '96', 'adult', 'Instead', 'culminating', 'been', '2015', 'heart', 'longtime', 'Kaley', 'media', 'Klingon', 'physicist', '147', 'tends', 'Thomas', 'explains', 'interested', 'Torresani', 'reluctantly', 'begins', 'Battlestar', 'bet', 'announced', 'minor', 'J.', 'four', 'In', 'several', 'website', '95', 'Arnold', 'Burton', 's', 'Thursdays', 'children', '75', 'store', 'fiction', 'interest', 'Reflection', 'writers', '52', 'clause', '100,000', '59', 'back-end', 'avoid', 'women', 'Reaction', 'prior', 'while', '121', 'Award', 'premiered', 'include', '2007', '30', 'greatest', '6.2/20', 'ultimatum', 'Next', 'clips', 'bandmate', 'smitten', 'then', 'together', 'specialist', 'proposal', '123', 'Fourth', 'start', 'sucked', 'Acquisition', 'versa', 'along', 'hit', 'trip', 'mixed', 'Emmy', 'vulnerable', 'acquired', 'revealed', '110', 'all', 'Warlords', 'Blinded', 'whiteboard', 'dates', 'Despite', 'Bang', 'including', 'consultants', 'freestyle', '140', 'turn', 'audience', 'physicists', 'significant', 'creators', 'confronted', 'Sheldon', 'Hits', 'waits', 'overly', '146', '135', '11', 'Tuesdays', 'Original', 'sales', 'lives', 'themselves', 'Catwoman', 'Acceleration', 'formation', 'separates', 'Fluctuation', 'gained', 'originally', 'experiment', 'moves', 'lead', 'created', 'boards', 'jealousy', '50,000', 'cluelessness', 'October', 'mild-mannered', '7', 'Pasadena', 'Spider-Man', 'California', 'International', 'attracted', 'reprising', 'title', 'Access', 'audiences', 'men', 'invited', 'favors', '279', 'retool'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#tokennize function\n",
    "def tokenize_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Tokenize the words using nltk's word_tokenize function\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "file_path = 'tbbt_wiki.txt'\n",
    "tokenized_words = tokenize_text(file_path)\n",
    "tbbt_vocab = set(tokenized_words)\n",
    "\n",
    "# print(\"vocabs:\", tbbt_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that search for the 10th most similar words of input word\n",
    "def search_similar(input):\n",
    "    word_dict={}\n",
    "    try:\n",
    "        #use glove model\n",
    "        input_embed = get_embed(model3, input)\n",
    "        for word in tbbt_vocab:\n",
    "            if word in vocabs:\n",
    "                word_dict[word] =  cos_sim(input_embed, get_embed(model3, word))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        sorted_dict = dict(sorted(word_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    except:\n",
    "        print('There is no word in the dictionary, please type new word')\n",
    "\n",
    "    return list(sorted_dict)[:10]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alternative', 'works', 'share', 'operated', 'won', 'renewed', 'could', 'many', 'else', '85']\n"
     ]
    }
   ],
   "source": [
    "#for example, I search the word 'city'\n",
    "x = search_similar('city')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the file path where you want to save the model parameters\n",
    "param_file_path = 'model_gl.bin'\n",
    "\n",
    "# Save only the parameters of the model in binary format\n",
    "torch.save(model3.state_dict(), param_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
